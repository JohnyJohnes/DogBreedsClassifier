{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Dog Breeds\n",
    "## Arsenii Belyakov\n",
    "### 450835"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uploading used libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
    "from keras_applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data (wrong version, idk why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14400\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "dataList = os.listdir(\"../stanford-dogs-dataset/images/Images/\") # os.listdir() method in python is used to get the list of all files and directories in the specified directory.\n",
    "numOfClasses = len(dataList)\n",
    "usedClasses = 3\n",
    "imageCounter = 0\n",
    "for dataClass in dataList:\n",
    "    imageCounter+=len(os.listdir(\"../stanford-dogs-dataset/images/Images/\"\n",
    "                     .format(dataClass))) \n",
    "print(imageCounter)\n",
    "print(numOfClasses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 breeds\n",
      "20580 images\n"
     ]
    }
   ],
   "source": [
    "# os.listdir() method in python is used to get the list of all files and directories in the specified directory.\n",
    "breed_list = os.listdir(\"../stanford-dogs-dataset/images/Images/\")\n",
    "usedBreeds = 3\n",
    "\n",
    "num_classes = len(breed_list)\n",
    "print(\"{} breeds\".format(num_classes))\n",
    "\n",
    "n_total_images = 0\n",
    "for breed in breed_list:\n",
    "    n_total_images += len(os.listdir(\"../stanford-dogs-dataset/images/Images/{}\".format(breed)))\n",
    "print(\"{} images\".format(n_total_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating directroies and subdirectories for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data',exist_ok=True) #Like mkdir(), but makes all intermediate-level directories needed to contain the leaf directory.\n",
    "os.makedirs('data/Train',exist_ok= True)\n",
    "os.makedirs('data/Validate',exist_ok= True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating subbdirectories for breeds, which will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usedClasses = 1\n",
    "\n",
    "for breed in breed_list:\n",
    "    os.makedirs('data/Train/'+breed,exist_ok= True)\n",
    "    os.makedirs('data/Validate/'+breed,exist_ok= True)\n",
    "    if usedClasses >= usedBreeds: #reached limit of used breads\n",
    "        break\n",
    "    usedClasses+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Save data into folders\n",
    "cpt - Conditional probability table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "validationToTraining = .1\n",
    "breed_in_use = 1\n",
    "\n",
    "for breed in os.listdir('data/Train/'):\n",
    "\n",
    "    # walk() generates the file names in a directory tree by walking the tree either top-down or bottom-up.\n",
    "    cpt = sum([len(files) for root, directory, files in os.walk('stanford-dogs-dataset/images/Images/{}/'.format(breed))])\n",
    "    validation = (int)(cpt*validationToTraining)\n",
    "    index = 0\n",
    "    \n",
    "    for file in os.listdir('../stanford-dogs-dataset/annotations/Annotation/{}'.format(breed)):\n",
    "        \n",
    "        img = Image.open('../stanford-dogs-dataset/images/Images/{}/{}.jpg'.format(breed, file))\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        if index<validation: #to make propper split\n",
    "            img.save('data/Validate/' + breed + '/' + file + '.jpg')\n",
    "        \n",
    "        else:\n",
    "            img.save('data/Train/' + breed + '/' + file + '.jpg')\n",
    "        index +=1\n",
    "        \n",
    "    if breed_in_use == usedBreeds: #check if we are still in our limit\n",
    "        break\n",
    "        \n",
    "    breed_in_use = breed_in_use+1\n",
    "            \n",
    "\n",
    "print(len(os.listdir('data/Train')))\n",
    "print(len(os.listdir('data/Validate')))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "## Build an input pipeline using the tf.data.Dataset API and/or keras.preprocessing.image API that loads the images, resizes them to the same resolution and applies some basic data augmentation like translation, rotation, etc. It should also prefetch the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
